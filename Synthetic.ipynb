{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq34yGVJcRw1"
   },
   "source": [
    "# Преамбула"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "J6Q56g0YbivF",
    "outputId": "4d2ed673-b445-4115-a52b-3ddc2c425a39"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MBYXUHPbkKq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8Lcy3BybmMU"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUG9c4cCbpcN"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTZI00Tobz-c"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Нормализация изображений: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMXTCQhocFJd"
   },
   "outputs": [],
   "source": [
    "def crop_pixels(x):\n",
    "    \"\"\"Обрезание значений пикселей нормированного изображения.\"\"\"\n",
    "    return min(1.0, max(0.0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHejQ8xUcHxB"
   },
   "outputs": [],
   "source": [
    "def imshow_array(array):\n",
    "    \"\"\"Отображение массива нормированных пикселей.\"\"\"\n",
    "    plt.axis('off')\n",
    "    plt.imshow((255.0 * array).astype(np.uint8), cmap=plt.get_cmap(\"gray\"), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbYtnpwQcKRz"
   },
   "outputs": [],
   "source": [
    "def dataset_Y_to_X(X, Y):\n",
    "    \"\"\"Поменять у датасета пары (X, Y) на (X, X) (нужно, например, для обучения автоэнкодера).\"\"\"\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msARXPM3cLZK"
   },
   "outputs": [],
   "source": [
    "def similarity_loss(y_true, y_pred):\n",
    "    \"\"\"Функция потерь, которая показала результаты лучше, чем MAE.\"\"\"\n",
    "    delta = tf.keras.backend.abs(y_true - y_pred)\n",
    "    squared = tf.keras.backend.square(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(delta - 0.5 * squared, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoB3A5hQHFY1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "info = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLiobj3f7_be"
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LfsdVkHO8BWB",
    "outputId": "f075fca5-4192-4c87-ff52-967c04613f47"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7FaXEE_8Cw1"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/MNIST-Information/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOluq8gIep6f"
   },
   "source": [
    "# Эксперимент для синтетических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5f0UYrRcMag"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HALYgpvf21V"
   },
   "outputs": [],
   "source": [
    "def norm_generator(size=1, loc=0, scale=1):\n",
    "    return sps.norm(loc=loc, scale=scale).rvs(size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a-v5hfnf4I2"
   },
   "outputs": [],
   "source": [
    "dataset_dim = 16 # Размерность данных.\n",
    "latent_dim  = 8  # Реальная (скрытая) размерность данных.\n",
    "final_noize_scale = 0.05 # Стандартное отклонение шума, складываемого с выходом функции.\n",
    "samples_number = 120000 # Размер выборки.\n",
    "tests_number   = 10000 # Размер тестовой выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW2lwh5m94t3"
   },
   "source": [
    "### Функции, задающие многообразия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ySDSye8hp-5"
   },
   "outputs": [],
   "source": [
    "def function_1(X, dataset_dim, latent_dim):\n",
    "    \"\"\"\n",
    "    Функция 1, задающая малоразмерное многообразие.\n",
    "    \"\"\"\n",
    "\n",
    "    Y = np.zeros(dataset_dim)\n",
    "\n",
    "    for i in range(dataset_dim):\n",
    "        Y[i] = np.sin(np.sin(2 * X[i % latent_dim]) + np.sin(2 * X[i // latent_dim]))\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBRDlpDa88g3"
   },
   "source": [
    "$$\n",
    "y = f_2(x): \\quad y_i = \\sin \\left ( \\sin (2 \\cdot x_{i \\; \\text{mod} \\; d}) + \\sin(2 \\cdot x_{[i / d]}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbd_0Cm58Sg4"
   },
   "outputs": [],
   "source": [
    "def function_2(X, dataset_dim, latent_dim):\n",
    "    \"\"\"\n",
    "    Функция 2, задающая малоразмерное многообразие.\n",
    "    \"\"\"\n",
    "\n",
    "    Y = np.zeros(dataset_dim)\n",
    "\n",
    "    for i in range(dataset_dim):\n",
    "        Y[i] = np.sin(np.sin(2 * X[i % latent_dim]) + np.sin(2 * X[i // latent_dim]))\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJK6MxoTgktS"
   },
   "source": [
    "$$\n",
    "y = f_3(x): \\quad y_i = \\sin \\left ( \\pi \\cdot \\tanh \\left [ \\left ( -3 x_{(i / d^2) \\; \\text{mod} \\; d}^3 + 2 x_{(i / d) \\; \\text{mod} \\; d}^2 + 4 x_{i \\; \\text{mod} \\; d} \\right ) / 10 \\right ] \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k8-AwaETeA_"
   },
   "outputs": [],
   "source": [
    "def function_3(X, dataset_dim, latent_dim):\n",
    "    \"\"\"\n",
    "    Функция 3, задающая малоразмерное многообразие.\n",
    "    \"\"\"\n",
    "\n",
    "    Y = np.zeros(dataset_dim)\n",
    "\n",
    "    for i in range(dataset_dim):\n",
    "        Y[i] = np.sin(1.0 * np.pi * np.tanh(0.1 * (-3.0 * X[(i // latent_dim**2) % latent_dim]**3 + 2.0 * X[(i // latent_dim) % latent_dim]**2 + 4.0 * X[i % latent_dim])))\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn-miZoXhhiT"
   },
   "outputs": [],
   "source": [
    "functions = [function_1, function_2, function_3]\n",
    "\n",
    "# НОМЕР ФУНКЦИИ.\n",
    "# #\n",
    "# #\n",
    "function_number = 3 - 1 # -1 для того, чтобы можно было нумеровать функции с единицы и не совершать глупых ошибок...\n",
    "# #\n",
    "# #\n",
    "\n",
    "func_path = path + \"Synthetic/function_\" + str(function_number + 1) + \"/\" + str(latent_dim) + \"_\" + str(dataset_dim) + '/' + (\"%.3e\" % final_noize_scale) + \"/\" + str(samples_number) + \"_\" + str(tests_number) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQTEAFdNhJwt"
   },
   "outputs": [],
   "source": [
    "def gen_samples(function, samples_number, dataset_dim, latent_dim, ln_scale = 1.0, fn_scale = 0.05):\n",
    "    \"\"\"\n",
    "    Генерация набора данных.\n",
    "    \"\"\"\n",
    "\n",
    "    # Шум во внутреннем представлении.\n",
    "    #np.random.seed(42)\n",
    "    W = norm_generator(loc=0.0, scale=ln_scale, size=(samples_number, latent_dim))\n",
    "    \n",
    "    # Отображение шума в пространство большей размерности.\n",
    "    base_sample = np.zeros((samples_number, dataset_dim))\n",
    "    for i in range(samples_number):\n",
    "        base_sample[i] = function(W[i], dataset_dim, latent_dim)\n",
    "    \n",
    "    # Дополнительный шум, накладываемый на итоговое многообразие.\n",
    "    noises_sample = norm_generator(loc=0, scale=fn_scale, size=(samples_number, dataset_dim))\n",
    "    sample = base_sample + noises_sample\n",
    "    \n",
    "    # Обрезание результатов.\n",
    "    #for i in range(samples_number):\n",
    "    #    for j in range(dataset_dim):\n",
    "    #        if sample[i][j] > 1:\n",
    "    #            sample[i][j] = 1\n",
    "    #        if sample[i][j] < -1:\n",
    "    #            sample[i][j] = -1\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFq9_79g-NmI"
   },
   "outputs": [],
   "source": [
    "samples = gen_samples(functions[function_number], samples_number, dataset_dim, latent_dim, fn_scale = final_noize_scale)\n",
    "test = gen_samples(functions[function_number], tests_number, dataset_dim, latent_dim, fn_scale = final_noize_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3brdSwtI-Ecp"
   },
   "source": [
    "## Автоэнкодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyvJbA9M592V"
   },
   "source": [
    "### Размерность кода и число эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5J56yqY-LyD"
   },
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    " \n",
    "codes_dim = 3\n",
    "\n",
    "# #\n",
    "# #\n",
    " \n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akIWS96r-S0q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "rel_path = func_path + \"Autoencoders/\" + str(codes_dim) + \"_\" + str(epochs) + \"/\"\n",
    "os.makedirs(rel_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCEbmnz3HVoW"
   },
   "outputs": [],
   "source": [
    "info['dataset_dim'] = dataset_dim\n",
    "info['latent_dim'] = latent_dim\n",
    "\n",
    "info['samples_number'] = samples_number\n",
    "info['tests_number'] = tests_number\n",
    "\n",
    "info['codes_dim'] = codes_dim\n",
    "info['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opP0MLLr-BvN"
   },
   "outputs": [],
   "source": [
    "def normal_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.05)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(64, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(16, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tf.keras.layers.Dense(dimension)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('sigmoid')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_layer = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(16, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(64, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 0 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(shape_input[0])(next_layer) # Подразумевается, что вход - всё равно вектор.\n",
    "    next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    \n",
    "    output_layer = next_layer\n",
    "\n",
    "    \n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_layer, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(lr = 1e-3)\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt, loss_weights = [1.0])\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEtKf4OCXVh5"
   },
   "outputs": [],
   "source": [
    "# Загрузка моделей.\n",
    "\n",
    "#encoder = tf.keras.models.load_model(rel_path + \"encoder.h5\")\n",
    "#decoder = tf.keras.models.load_model(rel_path + \"decoder.h5\")\n",
    "#autoencoder = autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "#with open(rel_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwIARIJ_-Mar"
   },
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = normal_autoencoder((dataset_dim,), codes_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ulVgf2n9-br5",
    "outputId": "356d3e73-65ff-4494-d0a1-4a2d6619f170"
   },
   "outputs": [],
   "source": [
    "history_callback = autoencoder.fit(samples, samples, epochs=epochs, validation_data=(test, test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVEjR0H0B753"
   },
   "outputs": [],
   "source": [
    "# Сохранение динамики loss-функции.\n",
    "loss_history = np.array(history_callback.history[\"loss\"])\n",
    "val_loss_history = np.array(history_callback.history[\"val_loss\"])\n",
    "\n",
    "np.savetxt(rel_path + \"loss.csv\", loss_history, delimiter=\"\\n\")\n",
    "np.savetxt(rel_path + \"val_loss.csv\", val_loss_history, delimiter=\"\\n\")\n",
    "\n",
    "last_n = 20\n",
    "last_loss = np.array(loss_history[-last_n:])\n",
    "last_loss_mean = np.mean(last_loss)\n",
    "last_loss_std = np.std(last_loss)\n",
    "\n",
    "last_val_loss = np.array(val_loss_history[-last_n:])\n",
    "last_val_loss_mean = np.mean(last_val_loss)\n",
    "last_val_loss_std = np.std(last_val_loss)\n",
    "\n",
    "info['last_loss_mean'] = last_loss_mean\n",
    "info['last_loss_std'] = last_loss_std\n",
    "info['last_val_loss_mean'] = last_val_loss_mean\n",
    "info['last_val_loss_std'] = last_val_loss_std\n",
    "\n",
    "with open(func_path + \"losses.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, last_loss_mean, last_loss_std])\n",
    "\n",
    "with open(func_path + \"val_losses.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, last_val_loss_mean, last_val_loss_std])\n",
    "\n",
    "# Сохранение моделей.\n",
    "autoencoder.save(rel_path + \"autoencoder.h5\")\n",
    "encoder.save(rel_path + \"encoder.h5\")\n",
    "decoder.save(rel_path + \"decoder.h5\")\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(rel_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3jwC8aO-rfp"
   },
   "source": [
    "### Получение кодов всех элементов датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffWE2hdK-tQX"
   },
   "outputs": [],
   "source": [
    "codes = np.array(encoder.predict(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wrrCFTw-0rm"
   },
   "outputs": [],
   "source": [
    "pca_codes_dim = codes_dim\n",
    "pca_codes = PCA(n_components=pca_codes_dim, whiten=True)\n",
    "codes_pca = np.array(pca_codes.fit_transform(codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8D9rMBS-40Z"
   },
   "source": [
    "### KDE для кодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muc1O4uDK5KQ"
   },
   "outputs": [],
   "source": [
    "# Загрузка параметров KDE.\n",
    "\n",
    "#with open(rel_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)\n",
    "\n",
    "#kde_codes = KernelDensity(bandwidth=info['bandwidth'], kernel='gaussian')\n",
    "#kde_codes.fit(codes_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwHNfqYs-1Vu"
   },
   "outputs": [],
   "source": [
    "def smart_gridsearch(begin, end, resolution = 7, rel_x_epsilon = 0.01, rtol = 0.001, n_jobs = 2, cv = 5):\n",
    "    while True:\n",
    "        grid = np.logspace(np.log10(begin), np.log10(end), resolution)\n",
    "        print(\"Поиск по сетке: \", grid)\n",
    "        params = {'bandwidth': grid}\n",
    "        \n",
    "        grid_search = GridSearchCV(KernelDensity(rtol = rtol), params, n_jobs = n_jobs, verbose = 10, cv = cv)\n",
    "        grid_search.fit(codes_pca)\n",
    "        \n",
    "        if grid_search.best_index_ == 0:\n",
    "            begin *= begin / end\n",
    "            end = grid[1]\n",
    "        elif grid_search.best_index_ == resolution - 1:\n",
    "            end *= end / begin\n",
    "            begin = grid[-2]\n",
    "        else:\n",
    "            begin = grid[grid_search.best_index_ - 1]\n",
    "            end = grid[grid_search.best_index_ + 1]\n",
    "\n",
    "            if end - begin < rel_x_epsilon * grid[grid_search.best_index_]:\n",
    "                return grid_search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4lzruwFmJRO9",
    "outputId": "e50a61b7-c758-4021-ccd7-5157e34dd927"
   },
   "outputs": [],
   "source": [
    "kde_codes = smart_gridsearch(0.05, 0.5).best_estimator_\n",
    "kde_codes.set_params(rtol = 0.0)\n",
    "print(kde_codes.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fac138z7J1rO"
   },
   "outputs": [],
   "source": [
    "info['bandwidth'] = kde_codes.get_params()['bandwidth']\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(rel_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp)\n",
    "\n",
    "with open(func_path + \"bandwidth.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, info['bandwidth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMW-XmnIJbUC"
   },
   "source": [
    "## Подсчёт энтропии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jRPs0RWJbtD"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy_monte_carlo(kde, N, random_state = 42):\n",
    "    samples  = kde.sample(N, random_state)\n",
    "    log_prob = np.array(kde.score_samples(samples))\n",
    "    \n",
    "    average = -math.fsum(log_prob) / N\n",
    "\n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_prob[i] - average)**2\n",
    "\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "\n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bzoYHiAMJjU_",
    "outputId": "31bdc8c7-d958-4e4f-d8fa-05e52f829fe6"
   },
   "outputs": [],
   "source": [
    "entropy, entropy_error = entropy_monte_carlo(kde_codes, len(codes_pca))\n",
    "entropy_error *= 3.3 # Коэффициент Стьюдента.\n",
    "print(\"H: %f, errH: %f\" % (entropy, entropy_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuDCxOcgHdcP"
   },
   "outputs": [],
   "source": [
    "info['MC entropy'] = entropy\n",
    "info['MC entropy error'] = entropy_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(rel_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCVMfzGzJkT0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def _lvo_step(bandwidth, samples, i):\n",
    "    lvo_samples = samples\n",
    "    np.delete(lvo_samples, i)\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    kde.fit(lvo_samples)\n",
    "    return kde.score_samples([samples[i]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWJJYhbkJlam"
   },
   "outputs": [],
   "source": [
    "def entropy_leave_one_out_parallel(bandwidth, samples, path, random_state = 42, first_N = None, parts = 10, recover_saved = False):\n",
    "    # Создание временных папок для сохранения прогресса.\n",
    "    parts_path = path + \"LOO_PARTS/\"\n",
    "    os.makedirs(parts_path, exist_ok=True)\n",
    "\n",
    "    # Если дано first_N, энтропия будет оцениваться только на первых first_N элементах.\n",
    "    N = 0\n",
    "    if first_N is None:\n",
    "        N = len(samples)\n",
    "    else:\n",
    "        N = first_N\n",
    "\n",
    "    # Число частей и массив, их содержащий.\n",
    "    N_per_part = N // parts\n",
    "    log_probs = []\n",
    "\n",
    "    # Восстанавливаем прогресс, если требуется.\n",
    "    recovered_parts = 0\n",
    "    if recover_saved:\n",
    "        for filename in os.listdir(parts_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                log_probs.append(np.loadtxt(parts_path + filename))\n",
    "                recovered_parts += 1\n",
    "\n",
    "    print(\"Восстановлено блоков данных: %d\" % recovered_parts)\n",
    "\n",
    "    # Подсчёт логарифма вероятности в точках.\n",
    "    for part in range(recovered_parts, parts):\n",
    "        log_probs.append(np.array(Parallel(n_jobs=2, verbose=10, batch_size=8)(delayed(_lvo_step)(bandwidth, samples, i) for i in range(part * N_per_part, min((part + 1) * N_per_part, N)))))\n",
    "        np.savetxt(parts_path + str(part) + \".csv\", log_probs[part], delimiter=\"\\n\")\n",
    "    \n",
    "    # Объединение в один массив.\n",
    "    log_prob = np.concatenate(log_probs)\n",
    "\n",
    "    # Суммирование и нахождение стандартного отклонения.\n",
    "    average = -math.fsum(log_prob) / N    \n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_prob[i] - average)**2\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "        \n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "65pweVPhJn54",
    "outputId": "360790e7-1a82-468a-d32d-44ef5e1bab73"
   },
   "outputs": [],
   "source": [
    "entropy, entropy_error = entropy_leave_one_out_parallel(kde_codes.get_params()['bandwidth'], codes_pca, rel_path, recover_saved = True)\n",
    "entropy_error *= 3.3 # Коэффициент Стьюдента.\n",
    "print(\"H: %f, errH: %f\" % (entropy, entropy_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fSDF8AeHgJm"
   },
   "outputs": [],
   "source": [
    "info['LOO entropy'] = entropy\n",
    "info['LOO entropy error'] = entropy_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(rel_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0y4Ajymc3tD"
   },
   "outputs": [],
   "source": [
    "with open(func_path + \"entropy.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, info['MC entropy'], info['MC entropy error'], info['LOO entropy'], info['LOO entropy error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyWwlKteJ0Rj"
   },
   "source": [
    "## Оценка размерности кодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76O2nG-oJ3WC"
   },
   "outputs": [],
   "source": [
    "tree_codes = BallTree(codes_pca, leaf_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vi8WlzWfKEtk"
   },
   "outputs": [],
   "source": [
    "def calc_pairs(tree, samples, radius):\n",
    "    total = sum(tree.query_radius(samples, r=radius, count_only=True)) - len(samples)\n",
    "    return total // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBx5nNRqKF4c"
   },
   "outputs": [],
   "source": [
    "max_pairs = len(codes_pca) * (len(codes_pca) - 1) // 2\n",
    "print(max_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTrZmwyU1Lad"
   },
   "outputs": [],
   "source": [
    "def ineq_binary_search(func, a, b, rel_eps = 0.01, max_a = None, min_b = None, verbose = 0):\n",
    "    while True:\n",
    "        print(\"Уточнение параметров: [%.3e, %.3e]\" % (a, b))\n",
    "        if (a != 0.0) and (not func(a)):\n",
    "            a /= 2.0\n",
    "        elif func(b):\n",
    "            b *= 2.0\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    while np.abs(1 - a / b) > rel_eps:\n",
    "        if verbose > 0:\n",
    "            print(\"Бинарный поиск: [%.3e, %.3e]\" % (a, b))\n",
    "        \n",
    "        pos = (a + b) / 2\n",
    "\n",
    "        if func(pos):\n",
    "            if (not (max_a is None)) and max_a < pos:\n",
    "                a = max_a\n",
    "                b = pos\n",
    "                break\n",
    "            a = pos\n",
    "\n",
    "        else:\n",
    "            if (not (min_b is None)) and min_b > pos:\n",
    "                a = pos\n",
    "                b = min_b\n",
    "                break\n",
    "            b = pos\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyqvE8RHdDpg"
   },
   "outputs": [],
   "source": [
    "# Начальное предположение.\n",
    "min_radius_a = 0.0\n",
    "min_radius_b = 0.1\n",
    "max_radius_a = 10.0\n",
    "max_radius_b = 20.0\n",
    "\n",
    "min_radius_a, min_radius_b = ineq_binary_search(lambda x: calc_pairs(tree_codes, codes_pca, x) < 1, min_radius_a, min_radius_b, min_b = 1.0e-07, verbose = 1)\n",
    "max_radius_a, max_radius_b = ineq_binary_search(lambda x: calc_pairs(tree_codes, codes_pca, x) < max_pairs, max_radius_a, max_radius_b, verbose = 1)\n",
    "\n",
    "min_radius = min_radius_a\n",
    "max_radius = max_radius_b\n",
    "\n",
    "#assert calc_pairs(tree_codes, codes_pca, min_radius) == 0\n",
    "#assert calc_pairs(tree_codes, codes_pca, max_radius) == max_pairs\n",
    "\n",
    "print(calc_pairs(tree_codes, codes_pca, min_radius_b))\n",
    "print(calc_pairs(tree_codes, codes_pca, max_radius_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VbR62RVc5Jx"
   },
   "outputs": [],
   "source": [
    "info['min_radius'] = min_radius\n",
    "info['max_radius'] = max_radius\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(rel_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvnkZNnSKeMY"
   },
   "outputs": [],
   "source": [
    "resolution = 64\n",
    "\n",
    "grid = np.logspace(np.log10(min_radius), np.log10(max_radius), resolution)\n",
    "pairs = np.zeros(resolution, dtype='int64')\n",
    "for i in range(resolution):\n",
    "    pairs[i] = calc_pairs(tree_codes, codes_pca, grid[i])\n",
    "    print(\"(%d/%d): %f, %d\" % (i+1, resolution, grid[i], pairs[i]))\n",
    "    \n",
    "    #if pairs[i] == max_pairs:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Id6xKE8-NvZ"
   },
   "outputs": [],
   "source": [
    "writer = csv.writer(open(rel_path + \"pairs.csv\", 'w'))\n",
    "for i in range(resolution):\n",
    "    writer.writerow([grid[i], pairs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8RxolgsOIbW"
   },
   "outputs": [],
   "source": [
    "log_grid__pairs = np.column_stack((grid, pairs))\n",
    "for i in range(resolution):\n",
    "    log_grid__pairs[i][0] = np.log(log_grid__pairs[i][0])\n",
    "    log_grid__pairs[i][1] = np.log(log_grid__pairs[i][1] / max_pairs)\n",
    "\n",
    "np.savetxt(rel_path + \"log_pairs.csv\", log_grid__pairs, delimiter=\",\", newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4nX3Wu5_XbT"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Synthetic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
